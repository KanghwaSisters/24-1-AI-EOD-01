# 24-1-AI-EOD-01
[ 24-1 /  AI EOD / Team 01 ]  
ğŸ‘©â€ğŸ’» ì´ìŠ¹ì—°, ë³€ì§€ì€


---
# 1. Environment
## State

- (`nrow`, `ncol`) ì°¨ì›ì˜ í–‰ë ¬ (type: `np.ndarray`)

| ìƒí™© | í‘œí˜„ |
| --- | --- |
| ì§€ë¢° | -2 |
| ê°€ë ¤ì§„ íƒ€ì¼ | -1 |
| ê·¸ ì™¸ | ì£¼ë³€ ì§€ë¢°ì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì (0~8) |

![environment1](environment1.png)

<br>
<br>

## Reward Design

### reward ì¡°ê±´

- ì§€ë¢° `mine`: ì§€ë¢°ë¥¼ ë°Ÿì€ ê²½ìš°
- ì„±ê³µ `clear`: ì§€ë¢°ë¥¼ ì œì™¸í•œ ëª¨ë“  ì¢Œí‘œê°€ ì—´ë¦° ê²½ìš°
- ì¤‘ë³µ í–‰ë™ `overlapped`: ìƒíƒœ ë§µì—ì„œ ì´ë¯¸ ì—´ë¦° í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê²½ìš°
- ì¶”ì¸¡ í–‰ë™ `guess`: ì£¼ë³€ì— ì—´ë¦° ì¢Œí‘œê°€ ì—†ëŠ”ë° ì„ íƒí•œ ê²½ìš° (ë‚´ì¥ í•¨ìˆ˜ë¡œ íŒë‹¨)
- ì¢‹ì€ í–‰ë™ `empty`: ìœ íš¨í•œ í–‰ë™. ì¶”ì¸¡ì´ë‚˜ ì¤‘ë³µì´ ì•„ë‹ˆê³  ì§€ë¢°ê°€ ì•„ë‹Œ í–‰ë™

![environment2](environment2.png)

<br>
<br>

### ì–‘ìˆ˜ ë³´ìƒ ì²´ê³„

|  | mine | clear | empty | overlapped | guess |
| --- | --- | --- | --- | --- | --- |
| reward | 0 | 0 | 1 | 0 | 0 |
| done | True | True | False | True | False |

- ì–‘ìˆ˜ ë³´ìƒ ì²´ê³„ì—ì„œëŠ” â€œì¤‘ë³µ ì„ íƒ ì‹œ ê²Œì„ ì¢…ë£Œâ€ ì¡°ê±´ì´ í•„ìˆ˜ì ì´ë‹¤. ë³´ìƒì„ í†µí•´ ì¤‘ë³µ í–‰ë™ì—ì„œ ë¹ ì ¸ë‚˜ì˜¤ê¸° ë§¤ìš° ì–´ë ¤ìš´ êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì´ë‹¤.

- ì´ˆë°˜ ë‚®ì€ ì„±ëŠ¥ì—ì„œ ì–´ëŠì •ë„ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ ê°€íŒŒë¥¸ ê¸°ìš¸ê¸°ë¡œ ìƒìŠ¹í•œë‹¤.

- ì–‘ìˆ˜ ë³´ìƒ ì²´ê³„ë¥¼ ì‚¬ìš©í•œ ì´ìœ 
    - ì¢‹ì€ í–‰ë™ì„ í•  ë•Œë§Œ ì–‘ìˆ˜ì˜ ë³´ìƒ â†’ ë³´ìƒì„ ìµœëŒ€í™” í•˜ë„ë¡ í•™ìŠµ â†’ ì„±ê³µë¥  ìƒìŠ¹
    - ì—ì´ì „íŠ¸ê°€ ì¢‹ì€ í–‰ë™ë§Œ í•˜ë„ë¡ í•™ìŠµí•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì„±ê³µë¥ ì´ ì˜¤ë¥¼ ê²ƒì´ë¼ê³  ìƒê°í–ˆë‹¤.

- ê°„ê³¼í•œ ì 
    - í´ë¦¬ì–´ë¥¼ í•  ë•Œì˜ ì´ë™ íšŸìˆ˜ê°€ ëª¨ë‘ ê°™ì§€ ì•ŠìŒ. ì¦‰, ë³´ìƒì´ ë†’ë‹¤ê³  í´ë¦¬ì–´í•˜ëŠ” ê²ƒë„ ì•„ë‹ˆê³  ë³´ìƒì´ ë‚®ë‹¤ê³  í´ë¦¬ì–´í•˜ì§€ ì•ŠëŠ” ê²ƒë„ ì•„ë‹ˆë‹¤.
    - í´ë¦¬ì–´í•˜ëŠ” ê²½ìš°ì— ë³´ìƒì„ ì¶”ê°€ë¡œ ë” í¬ê²Œ ì£¼ëŠ” ê²ƒìœ¼ë¡œ ìœ„ì˜ ë¬¸ì œ ìƒì‡„ ì‹œë„
    â†’ í•˜ì§€ë§Œ ì´ë™ íšŸìˆ˜ì˜ ì°¨ì´ë¡œ ì¸í•´ ìµœëŒ€ ë³´ìƒì˜ í¸ì°¨ê°€ ì¡´ì¬í•œë‹¤.
    - ê²°ì •ì ìœ¼ë¡œ â€œì¤‘ë³µ ì„ íƒ ì‹œ ê²Œì„ ì¢…ë£Œâ€ë¼ëŠ” ì¡°ê±´ì´ í•™ìŠµ ì†ë„ë¥¼ ë„ˆë¬´ ëŠë¦¬ê²Œ í•˜ê³ , ì§€ë¢°ì°¾ê¸° ê²Œì„ê³¼ë„ ë§ì§€ ì•ŠëŠ” ì¡°ê±´ì´ë‹¤.

<br>

### ìŒìˆ˜ ë³´ìƒ ì²´ê³„

|  | mine | clear | empty | overlapped | guess |
| --- | --- | --- | --- | --- | --- |
| reward | -1 | 1 | 1 | -1 | 0.3 |
| done | True | True | False | False | False |

- í•™ìŠµ ì´ˆë°˜ë¶€í„° ì¼ì •í•œ ê¸°ìš¸ê¸°ë¡œ ì„±ëŠ¥ì´ í–¥ìƒí•œë‹¤.

- ì§€ë¢°ì˜ ë³´ìƒì„ ì•„ì£¼ ë‚®ê²Œ ì£¼ëŠ” ê²ƒì€ ì˜ë¯¸ê°€ ì—†ë‹¤. ì–´ì°¨í”¼ ì§€ë¢°ë¥¼ ë°Ÿìœ¼ë©´ ëì´ê¸° ë•Œë¬¸ì´ê³ , ì—ì´ì „íŠ¸ê°€ í•™ìŠµí•  ë•Œ ê·¹ë‹¨ê°’ì´ ìƒê²¨ í•™ìŠµì— í˜¼ëˆì´ ìƒê¸´ë‹¤. ê²Œì„ ì¢…ë£Œ ì‹œì˜ ë³´ìƒë³´ë‹¤ëŠ” â€œì¢‹ì€ í–‰ë™â€ì„ ë§ì´ í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤.

- ë³´ìƒë“¤ì˜ ë¹„ìœ¨
    - ê²½í—˜ ìƒ ì§€ë¢°ì™€ í´ë¦¬ì–´ì˜ ë³´ìƒì€ ë¹„ìœ¨ì„ ë§ì¶”ëŠ” ê²ƒì´ ì¢‹ë‹¤.
    - í•˜ì§€ë§Œ ê²Œì„ì„ ì§„í–‰í•˜ëŠ” ë™ì•ˆ ë°œìƒí•˜ëŠ” ì¤‘ë³µ, ì¢‹ì€ í–‰ë™, ì¶”ì¸¡ ë³´ìƒì˜ ë¹„ìœ¨ì„ ë§ì¶”ê¸°ë³´ë‹¤ëŠ” ì¤‘ë³µ í–‰ë™ì´ í•™ìŠµì— ìˆì–´ì„œ ê°€ì¥ í° ë¬¸ì œì ì´ê¸° ë•Œë¬¸ì— ì§€ë¢°ì™€ ê°™ì€ ê°€ì¥ ë‚®ì€ ë³´ìƒì„ ì£¼ì—ˆë‹¤.
    - ì¢‹ì€ í–‰ë™ê³¼ í´ë¦¬ì–´ëŠ” ëª¨ë‘ ìœ ë„í•´ì•¼í•  í–‰ë™ì´ë¯€ë¡œ ê°€ì¥ í° ë³´ìƒì„ ì£¼ì—ˆë‹¤.
    - ì¶”ì¸¡í•œ í–‰ë™ì˜ ê²½ìš° ì§€ë¢°ë‚˜ ì¤‘ë³µë³´ë‹¤ëŠ” ë‚˜ì€ í–‰ë™ì´ê³ , ì¶”ì¸¡í•œ í–‰ë™ì„ í†µí•´ ìš´ì´ ì¢‹ê²Œ íŒì´ ì—´ë¦´ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì‘ì€ ì–‘ìˆ˜ì˜ ë³´ìƒì„ ì£¼ì—ˆë‹¤.

- í´ë¦¬ì–´ ë³´ìƒì˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ì•ˆëœë‹¤ê³  ìƒê°í•œë‹¤.(ì‚¬ê³ ì‹¤í—˜)
    - í´ë¦¬ì–´ ë³´ìƒì˜ í¬ê¸°ê°€ ë‹¤ë¥¸ ë³´ìƒë³´ë‹¤ë„ ë„ˆë¬´ í° ê²½ìš°, ë‹¤ë¥¸ ì–´ë–¤ í–‰ë™ì„ í•´ì„œë¼ë„ í´ë¦¬ì–´ë§Œ í•˜ë©´ ë³´ìƒì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ìœ¼ë¡œ ì›€ì§ì´ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ì¦‰, ì¢‹ì€ í–‰ë™ì„ í•  ìœ ë„ê°€ ì‘ì•„ì§„ë‹¤.

- ì „ì²´ì ìœ¼ë¡œ ë³´ìƒì˜ í¬ê¸°ê°€ í¬ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ë‹¤. ë”°ë¼ì„œ ëª¨ë‘ -1~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì„¤ì •í–ˆë‹¤.

<br>

### ì–‘ìˆ˜ ë³´ìƒ ì²´ê³„ vs. ìŒìˆ˜ ë³´ìƒ ì²´ê³„

![environment3](environment3.png)

- ìŒìˆ˜ ë³´ìƒ ì²´ê³„ì¸ ê²½ìš°ì— ì¼ì •í•œ ê¸°ìš¸ê¸°ë¡œ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¸ë‹¤.

<br>
<br>

## ì†ë„ ê°œì„ 

- list ìë£Œí˜• ëŒ€ì‹  np.array ìë£Œí˜• ì‚¬ìš©
- forë¬¸ ìµœì†Œí™” â†’ numpy í•¨ìˆ˜ í™œìš©
- action: index (0~`nrow*ncol`-1)
    - `divmod()` í•¨ìˆ˜ë¥¼ í†µí•´ ì¢Œí‘œë¡œ ë°”ê¿”ì„œ ì‚¬ìš©

<br>
<br>

## Render

- pandas.DataFrameìœ¼ë¡œ ë§µ ì‹œê°í™”
- `render_answer()`, `render(state)` í•¨ìˆ˜ë¡œ êµ¬í˜„
- `render_color()` í•¨ìˆ˜ë¡œ ìˆ«ìë³„ ìƒ‰ ì ìš©

![environment4](environment4.png)

<br>
<br>

---

# 2. DQN Net

## Input

- `state_size`: ìƒíƒœ ì‚¬ì´ì¦ˆ (nrow*ncol)
- `action_size`: action ê°œìˆ˜, ë§ˆì§€ë§‰ ì „ì—°ê²° ì¸µì˜ output ì°¨ì› (nrow*ncol)
- `conv_units`: ê° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ì±„ë„ ê°œìˆ˜ (64)

<br>

## ì¼ë°˜ CNN

![net1](net1.png)

```markdown
# í•©ì„±ê³± ì¸µ
self.conv1 = nn.Conv2d(in_channels=1, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=2)
self.conv2 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
self.conv3 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
self.conv4 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)

# ì „ì—°ê²° ì¸µ
self.fc_size = conv_units * (state_size[-1]+2) * (state_size[-2]+2)
self.fc = nn.Linear(self.fc_size, action_size)
```

```markdown
# ìˆœì „íŒŒ
x = F.relu(self.conv1(x))
x = F.relu(self.conv2(x))
x = F.relu(self.conv3(x))
x = F.relu(self.conv4(x))

# flatten
x = x.view(-1, self.fc_size)

# ì™„ì „ ì—°ê²°ì¸µ
x = self.fc(x)
```

- ì°¨ì› ( C: conv_unit, A: action_size )
    
    
    | input | conv 1 (padding=2) | conv 2 | conv 3 | conv 4 | flatten | fc (output) |
    | --- | --- | --- | --- | --- | --- | --- |
    | 1x9x9 | C x 11 x 11 | C x 11 x 11 | C x 11 x 11 | C x 11 x 11 | 1 x C*11*11 | 1 x A |
- í™œì„±í™” í•¨ìˆ˜: ReLU
- ì´ 4ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ + 1ê°œì˜ ì „ì—°ê²° ë ˆì´ì–´
    - bias = False
    - kernel_size = (3,3)
    - padding = 2 â†’ 1 ( ì²« ë²ˆì§¸ë§Œ 2)

ê° ì¢Œí‘œì˜ ì£¼ë³€ 8ì¹¸ì˜ ì •ë³´ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì§€ë¢°ì°¾ê¸° ê²Œì„ì˜ íŠ¹ì„±ì„ ì‚´ë¦¬ê¸° ìœ„í•´ kernel sizeëŠ” (3,3)ìœ¼ë¡œ ê³ ì •í•˜ì˜€ë‹¤. ê°™ì€ ì´ìœ ë¡œ paddingë„ ì²˜ìŒì—ë§Œ 2ê°œë¡œ í•´ì„œ ê°€ì¥ìë¦¬ê¹Œì§€ (3,3)ì˜ í•„í„°ê°€ ì˜ íƒìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•˜ê³ , ë”ì´ìƒ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¬ì§€ ì•Šì•˜ë‹¤.

<br>

---

## CNN + Batch Normalize

![net2](net2.png)

```markdown
# í•©ì„±ê³± ì¸µ
## ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
self.conv1 = nn.Conv2d(in_channels=1, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=2)
## ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ + ë°°ì¹˜ ì •ê·œí™”
self.conv2 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
self.bn2 = nn.BatchNorm2d(conv_units)
self.conv3 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
self.bn3 = nn.BatchNorm2d(conv_units)
## ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
self.conv4 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)

# ì „ì—°ê²° ì¸µ
self.fc_size = conv_units * (state_size[-1]+2) * (state_size[-2]+2)
self.fc = nn.Linear(self.fc_size, action_size)
```

```markdown
# ìˆœì „íŒŒ
x = F.relu(self.conv1(x))
x = F.relu(self.bn2(self.conv2(x)))
x = F.relu(self.bn3(self.conv3(x)))
x = F.relu(self.conv4(x))

# flatten
x = x.view(-1, self.fc_size)

# ì™„ì „ ì—°ê²°ì¸µ
x = self.fc(x)
```

- í™œì„±í™” í•¨ìˆ˜: ReLU
- í•©ì„±ê³± ì¸µì€ ìœ„ì™€ ë™ì¼
- 2, 3ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ â†’ ë°°ì¹˜ ì •ê·œí™” â†’ í™œì„±í™” í•¨ìˆ˜

<br>

### Batch Normalization

- í•™ìŠµ ê³¼ì •ì—ì„œ ê° ë°°ì¹˜ ë‹¨ìœ„ ë³„ ë‹¤ì–‘í•œ ë¶„í¬ë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ **ê° ë°°ì¹˜ë³„ë¡œÂ í‰ê· ê³¼Â ë¶„ì‚°ì„ ì´ìš©í•´Â ì •ê·œí™”**í•˜ëŠ” ê²ƒ
- **ì‹ ê²½ë§ ì•ˆì— í¬í•¨**ë˜ì–´ í•™ìŠµì‹œ í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ê³¼ì •
- í™œì„±í™” í•¨ìˆ˜ ì•ì— ì ìš©
- í‰ê·  0, í‘œì¤€ í¸ì°¨ 1ì¸ ê°€ìš´ë°ë¡œ ê°’ë“¤ì„ ë¿Œë ¤ì£¼ê¸° ë•Œë¬¸ì—, ì…ë ¥ ê°’ë“¤ì— ëŒ€í•œ update í•´ì•¼í•˜ëŠ” í¸ì°¨ë“¤ì´ í¬ì§€ ì•ŠëŠ”ë‹¤. ì¦‰, Learning rateë¥¼ í¬ê²Œ í•´ë„ ìƒê´€ì—†ë‹¤. â†’ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ í•™ìŠµ ê°€ëŠ¥
- Regularization íš¨ê³¼ê°€ ìˆê¸° ë•Œë¬¸ì— dropoutì„ ì•ˆí•´ë„ ë˜ëŠ” ì¥ì 

> [train ë‹¨ê³„ì—ì„œì˜ ìˆ˜ì‹]
<br>
$BN(X) = \gamma({{X-\mu batch}\over {\sigma batch}}) + \beta$

- $X$: ì…ë ¥ ë°ì´í„°
- $\gamma$: ì¶”ê°€ ìŠ¤ì¼€ì¼ë§
- $\beta$: í¸í–¥
- $\mu batch = {1\over B}\sum_{i}x_i$: ë°°ì¹˜ ë³„ í‰ê· ê°’
- $\sigma batch = {1\over B}\sum_{i}(x_i- \mu batch)^2$: ë°°ì¹˜ ë³„ í‘œì¤€ í¸ì°¨

ì—¬ê¸°ì„œ $\beta, \gamma$ëŠ” í•™ìŠµí•˜ëŠ” íŒŒë¼ë¯¸í„°ì´ë‹¤. ì´ íŒŒë¼ë¯¸í„°ëŠ” ì‹ ê²½ë§ì˜ non-linearityë¥¼ ìœ ì§€í•˜ë„ë¡ í•´ì¤€ë‹¤.

$\beta$ê°€ ìˆê¸° ë•Œë¬¸ì— ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ê²½ìš° ì‹ ê²½ë§ì—ì„œ í¸í–¥ì„ ë„£ì„ í•„ìš”ê°€ ì—†ë‹¤.

> [test ë‹¨ê³„ì—ì„œì˜ ìˆ˜ì‹]
<br>
$BN(X) = \gamma({{x-\mu BN}\over {\sigma BN}}) + \beta$

- $\mu BN = {1\over N}\sum_i \mu^i batch$
- $\sigma BN = {1\over N}\sum_i \sigma ^i batch$

ì¶”ë¡  ë‹¨ê³„ì— Batch Normalizationì„ ì ìš©í•  ë•ŒëŠ”Â **í•™ìŠµ ë‹¨ê³„ì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì €ì¥í•œ ê°’**ì„ ì´ìš©í•´ ì •ê·œí™”í•œë‹¤.

ëª¨ì§‘ë‹¨ ì¶”ì • ë°©ì‹ ë˜ëŠ” ì´ë™í‰ê· (Moving average) ë°©ì‹ì„ ì´ìš©í•´ êµ¬í•œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³ ì •ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ëª¨ì§‘ë‹¨ ì¶”ì • ë°©ì‹ì˜ ê²½ìš° ëª¨ë“  mini batchì˜ í‰ê· , ë¶„ì‚° ê°’ì„ ì €ì¥í•˜ê³  ìˆì–´ì•¼ í•˜ë¯€ë¡œ ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ ì£¼ë¡œ ì´ì „ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ì§‘ë‹¨ì˜ ì •ë³´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì¸ Moving averageë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

ì°¸ê³ : [https://velog.io/@js03210/Deep-Learning-Batch-Normalization-ë°°ì¹˜-ì •ê·œí™”](https://velog.io/@js03210/Deep-Learning-Batch-Normalization-%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94)

<br>

- - -

## ì‹œí–‰ì°©ì˜¤

### pooling

ì²˜ìŒì—ëŠ” Maxpoolingì„ ì ìš©í–ˆì—ˆë‹¤. í•˜ì§€ë§Œ Maxpoolingì„ ì ìš©í•œ ê²½ìš° í•™ìŠµì´ ì „í˜€ ë˜ì§€ ì•Šì•„ ì‚­ì œí–ˆë‹¤. ì•„ë§ˆë„ ë§µ ì‚¬ì´ì¦ˆê°€ 9x9ë°–ì— ë˜ì§€ ì•Šê³ , paddingìœ¼ë¡œ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë ¤ë„ 11x11ì´ê¸° ë•Œë¬¸ì— Maxpoolingì„ ì ìš©í•˜ë©´ ì‚¬ì´ì¦ˆê°€ 5x5ë°–ì— ë˜ì§€ ì•Šì•„ ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ë‹¤ê³  ìƒê°í–ˆë‹¤.

<br>

### Convolution filter ê°œìˆ˜

ì´ ì‹ ê²½ë§ì—ì„œëŠ” ì‚¬ìš©í•œ Convolution layer 4ê°œì˜ filter ê°œìˆ˜ê°€ ëª¨ë‘ ì…ë ¥ë°›ì€ unit sizeë¡œ ê³ ì •ì´ë‹¤. filter ê°œìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ë§ì€ íŠ¹ì§•ì„ ì¶œë ¥í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ê³„ì‚°ëŸ‰ì´ ë§ì•„ì ¸ ì†ë„ê°€ ëŠë ¤ì§€ëŠ” ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.

ê²°ë¡ ì ìœ¼ë¡œëŠ” ë‚˜ë¨¸ì§€ ìš”ì¸ì´ ê±°ì˜ ê°™ì€ ìƒí™©ì—ì„œ conv_unit=64ì¸ ëª¨ë¸ê³¼ 128ê°œì¸ ëª¨ë¸ ì¤‘ 64ê°œì¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë” ë†’ê²Œ ë‚˜ì™”ë‹¤. ëª¨ë‘ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ì‹ ê²½ë§ì´ì—ˆê¸° ë•Œë¬¸ì— ì´ë¯¸ ì†ë„ê°€ ëŠë ¤ì§„ ìƒíƒœë¼ì„œ 128ê°œì¸ ê²½ìš° ì†ë„ê°€ ë„ˆë¬´ ëŠë ¤ì¡Œê¸° ë•Œë¬¸ì¼ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•œë‹¤. 

![net3](net3.png)

<br>
<br>

---
---

# 3. DQN Agent

## Input

- `env`: í™˜ê²½. í™˜ê²½ ê´€ë ¨ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜¬ ë•Œ ì‚¬ìš©

<br>

---

## get_action

epsilon íƒí—˜ì„ ì‚¬ìš©í•´ actionì„ ì„ íƒí•œë‹¤. epsilonì˜ í™•ë¥ ë¡œ ëœë¤ actionì„ ì„ íƒí•˜ë©° íƒí—˜í•˜ê³ , ë‚˜ë¨¸ì§€ í™•ë¥ ë¡œ ëª¨ë¸ì—ì„œ ê³„ì‚°í•œ q valueì˜ ìµœëŒ“ê°’ì„ actionìœ¼ë¡œ ì„ íƒí•œë‹¤.

> ì¤‘ë³µ í–‰ë™ì´ ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œëŠ” í•™ìŠµ ì¤‘ ë¬´í•œ ì—í”¼ì†Œë“œì— ë¹ ì§€ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•˜ê³ ì ìµœì†Œ epsilonê°’ì„ 0.01(1%)ë¡œ ì„¤ì •í•´ ë¹„êµì  ë¹ ë¥´ê²Œ ë¬´í•œ ì—í”¼ì†Œë“œì—ì„œ ë¹ ì ¸ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ í–ˆë‹¤. ëŒ€ì‹  epsilon decayê°’ì„ í¬ê²Œ ì„¤ì •í•´ ê½¤ ì˜¤ëœ ì—í”¼ì†Œë“œ ë™ì•ˆ íƒí—˜í•˜ë„ë¡ í–ˆë‹¤.
> 

### gpu

- í™˜ê²½ì—ì„œ ë°›ì€ `state`ë¥¼ `torch.tensor` íƒ€ì…ìœ¼ë¡œ ë°”ê¾¸ê³ , gpuì— ì˜¬ë¦¼
- q valueë¥¼ model(gpu)ì—ì„œ ê³„ì‚°í•œ ë’¤, ìµœëŒ€ qê°’ì„ ì°¾ëŠ” ê³¼ì •ì€ cpuì—ì„œ ê³„ì‚° (`torch.argmax()` í•¨ìˆ˜ ì‚¬ìš© ì‹œ gpuì—ì„œ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ)

### ì •ê·œí™”

êµ¬í˜„í•œ í™˜ê²½ì—ì„œì˜ stateì˜ ìµœëŒ“ê°’ì´ 8ì´ë¯€ë¡œ ì…ë ¥ë°›ì€ stateë¥¼ 8ë¡œ ë‚˜ëˆ  ê°„ë‹¨í•˜ê²Œ ì •ê·œí™”í–ˆë‹¤.

- ì •ê·œí™”ë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ ë°ì´í„°ì˜ í¬ê¸°ê°€ ë“¤ì‘¥ë‚ ì‘¥í•´ ëª¨ë¸ì´ ë°ì´í„° ê°„ í¸ì°¨ê°€ í° Feature ìœ„ì£¼ë¡œ í•™ìŠµì´ ì§„í–‰ë˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì´ìƒí•˜ê²Œ í•´ì„í•  ìš°ë ¤ê°€ ìˆë‹¤.

### ì°¨ì› ë¬¸ì œ

batch normalizationì„ ì ìš©í•œ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ì°¨ì›ì˜ ì²« ë²ˆì§¸ ì›ì†Œê°€ batch sizeì´ê¸° ë•Œë¬¸ì— actionì„ ì„ íƒí•  ë•Œ ì°¨ì›ì´ ë§ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ batch normalizationì„ ì ìš©í•˜ì§€ ì•Šì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•  ë•Œë³´ë‹¤ `.unsqueeze(0)`ë¥¼ í•œ ë²ˆ ë” ì ìš©í•´ ì°¨ì›ì„ ë§ì¶°ì¤€ë‹¤.

<br><br>

---

## DQN ì•Œê³ ë¦¬ì¦˜

### replay memory

```python
self.memory = deque(maxlen = MEM_SIZE)
```

- `self.append_sample()` í•¨ìˆ˜ë¥¼ í†µí•´ replay memoryì— ìƒ˜í”Œ ì¶”ê°€
    - í•œ ìƒ˜í”Œ: (state, action, reward, next_state, done, clear)

### model, target model

```python
self.model = DQN_Net(state_size, len(action), CONV_UNITS).to(self.device)
self.target_model = DQN_Net(state_size, len(action), CONV_UNITS).to(self.device)
self.update_target_model()
self.criterion = nn.MSELoss()
```

- `self.update_target_model()` í•¨ìˆ˜ë¥¼ í†µí•´ ì¼ì • ì£¼ê¸°ë§ˆë‹¤ target model ì—…ë°ì´íŠ¸
- ì˜¤ë¥˜ í•¨ìˆ˜: nn.MSELoss()

model(DQN ì‹ ê²½ë§)ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [2. DQN ì‹ ê²½ë§](#-dqn-ì‹ ê²½ë§)

<br><br>

### train_model()

replay memoryì—ì„œ BATCH_SIZE(64)ë§Œí¼ mini batchë¥¼ ë½‘ì•„ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤. stateì™€ next stateì˜ ë°°ì¹˜ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê³¼ì •ì—ì„œ state ì •ê·œí™”ë¥¼ í•œë‹¤.

> ë¯¸ë‹ˆë°°ì¹˜ì—ì„œ ê° ìš”ì†Œë“¤ì„ ê°€ì ¸ì™€ `np.ndarray` íƒ€ì…ì„ `torch.tensor` íƒ€ì…ìœ¼ë¡œ ë§Œë“œëŠ” ê³¼ì •ì—ì„œ list complicationì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì†ë„ë¥¼ ëŠë¦¬ê²Œ í•œë‹¤. ì• ì´ˆì— í™˜ê²½ì—ì„œ `np.ndarray` íƒ€ì… ëŒ€ì‹  `torch.tensor` íƒ€ì…ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤ ì¤‘ì´ë‹¤.
> 

í•™ìŠµ ì‹œì‘ ì´í›„ë¡œ í•™ìŠµì„ í•  ë•Œë§ˆë‹¤ epsilonì„ epsilon decay ë§Œí¼ ê°ì†Œì‹œí‚¨ë‹¤.

[ í•™ìŠµ ê³¼ì • ]

```python
pred_q_values = self.model(states).gather(1, actions) # action idxì˜ ë°ì´í„°ë§Œ êº¼ëƒ„
```

- ì˜ˆì¸¡ ê°’ìœ¼ë¡œ Q(s, a)ê°’ ì‚¬ìš© (ì°¨ì›: (batch size, num actions))

```python
with torch.no_grad():
    next_q_values = self.target_model(next_states).max(1).values.reshape(-1,1)
    target_q_values = rewards + (torch.ones(next_q_values.shape, device=self.device) - dones) * self.gamma * next_q_values
```

- íƒ€ê²Ÿ ê°’ ê³„ì‚°: $reward + (1-done)\times gamma\times Q(s', a')$

```python
loss = F.mse_loss(pred_q_values, target_q_values)
```

- ì˜¤ë¥˜ í•¨ìˆ˜ (mse)ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸

```python
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
```

- ì—­ì „íŒŒ ê³„ì‚° í›„ ê¸°ìš¸ê¸°ë¥¼ optimizerì— ë§ì¶° ê°€ì¤‘ì¹˜ ìˆ˜ì •

<br><br>

---

## Optimizer

Adamì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 

### SGD vs. Adam

> í•™ìŠµì´ ë˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ê²Œì„íŒì„ 10ê°œë¡œ ì œí•œí•œ í™˜ê²½ì—ì„œ ë¹„êµí•´ë³´ì•˜ë‹¤.
> 

RAdamë„ ì‹¤í—˜í•´ë³´ì•˜ì§€ë§Œ ê²Œì„íŒ 10ê°œ ì œí•œ í™˜ê²½ì„ì—ë„ ì „í˜€ í•™ìŠµì´ ë˜ì§€ ì•Šì•˜ë‹¤.

[ SGD ]

ì´í›„ ì¶”ê°€.

[ Adam ]

ì´í›„ ì¶”ê°€.

---

# Rule based

- í•œë²ˆ ë°Ÿì€ ê³³ì€ ë‹¤ì‹œ ë°Ÿì§€ ì•ŠëŠ” ê·œì¹™ì„ ê¸°ê³„ì—ê²Œ ì•Œë ¤ì¤€ë‹¤.

## 1. Environment
### Reward Design
---

- ì§€ë¢° `mine`: ì§€ë¢°ë¥¼ ë°Ÿì€ ê²½ìš°
- ì„±ê³µ `clear`: ì§€ë¢°ë¥¼ ì œì™¸í•œ ëª¨ë“  ì¢Œí‘œê°€ ì—´ë¦° ê²½ìš°
- `empty`: ì§€ë¢°ê°€ ì•„ë‹Œ í–‰ë™
```python
  self.reward_dict = {'mine': -8, 'empty':1, 'clear':5}
```

#### ì–‘ìˆ˜ ë³´ìƒ ì²´ê³„

|  | clear | empty |  
| --- | --- | --- |
| reward | 5 | 1 | 
| done | True | False | 

- Rule based ì—ì„œëŠ” ì¤‘ë³µ ì„ íƒì„ ë§ˆìŠ¤í‚¹ì„ í•¨ìœ¼ë¡œì¨ ë§‰ëŠ”ë‹¤.

- empty reward
  - ver1. í•œ ì—í”¼ì†Œë“œ ë‹¹, ëˆ„ë¥´ëŠ” íšŸìˆ˜ê°€ 9ë²ˆ ì´ìƒ í˜¹ì€ ì•ˆ ëˆŒë¦° ë²„íŠ¼ì´ 30ê°œ ë¯¸ë§Œì¸ ê²½ìš°, machine ì€ ì—´ë ¤ìˆëŠ” íŒì˜ ì •ë³´ë¥¼ í† ëŒ€ë¡œ ì§€       ì ì¸ ì¶”ë¡ ì„ í•´ì•¼í•œë‹¤ê³  íŒë‹¨í•˜ì˜€ë‹¤. ë”°ë¼ì„œ ìœ„ì˜ 2ê°€ì§€ ê²½ìš°, í•œë²ˆ ëˆŒë €ì„ ë•Œ í„°ì§€ëŠ” íšŸìˆ˜ë¥¼ `empty` ë³´ìƒì— ê³±í•˜ì—¬ ë” í° ë³´ìƒì„ ì£¼ì—ˆë‹¤.

      ```python
      reward = self.reward_dict['empty']
            if((self.totalcnt>8) or (np.sum(self.present_state == -2) < 30)):
                reward = self.reward_dict['empty'] * switch_cnt
            else:
                reward = self.reward_dict['empty']
            done = False
      ```
      
    - ê°„ê³¼í•œ ì : ìœ„ì˜ ë‘ ê°€ì§€ì˜ ê²½ìš°, í•œ ê²Œì„ì—ì„œ ì´ë¯¸ ë§ì€ ë²„íŠ¼ì„ ëˆ„ë¥¸ ìƒíƒœì´ê¸° ë•Œë¬¸ì— ì¢‹ì€ íŒë‹¨ì„ í•œë‹¤ê³  í•´ì„œ ì—¬ëŸ¬ ì¢Œí‘œê°€ ê°™ì´ í„°ì§€ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. ì˜¤íˆë ¤ ë‚˜ì¤‘ì— ë°ŸëŠ” ì¢Œí‘œëŠ” ì ê²Œ í„°ì§ˆ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. ë”°ë¼ì„œ `empty` ë³´ìƒì— switch_cnt ë¥¼ ê³±í•˜ëŠ” ê²ƒì´ ì§€ì ì¸ í™œë™ì„ í•™ìŠµì‹œí‚¤ëŠ”ë° ë¶€ì ì ˆí•˜ë‹¤ê³  íŒë‹¨í•˜ì˜€ë‹¤.
          
  - ver2. í•œ ì—í”¼ì†Œë“œ ë‹¹, ë§Œì•½ ì´ë¯¸ í„°ì§„ ê³³ì˜ ì˜†ìë¦¬(ëˆ„ë¥¸ ê³³ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ 3\*3 í–‰ë ¬) ê°€ ì´ë¯¸ í„°ì ¸ìˆì„ ë•Œ, ê¸°ê³„ëŠ” ì¶”ë¡ ì„ í•œ ê²ƒì´ë¯€ë¡œ ë” í° ë³´ìƒì„ ì£¼ë„ë¡ í•˜ì˜€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì˜ ì‚¬ì§„ì—ì„œ ë¹¨ê°„ ì ì„ ëˆ„ë¥¸ ë‹¤ê³  í–ˆì„ ë•Œ, ë¹¨ê°„ ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ 3*3 gird_world ë¥¼ íƒìƒ‰í•œë‹¤. ì´ ë•Œ, ì´ë¯¸ íƒìƒ‰ ëœ ê²ƒ(ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œëŠ” (2,3)) ì´ ìˆë‹¤ë©´ 3ì˜ ë³´ìƒì„ ì£¼ì—ˆë‹¤. <br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/18146fec-7aef-4ccc-8629-6d861a8b57bc" width="300">
</p>

      ```python
            reward = self.reward_dict['empty']
                pred = 0
                directions = [(-1, 0), (1, 0), (0, -1), (0, 1),
                        (-1, -1), (-1, 1), (1, -1), (1, 1)]
                for dx, dy in directions:
                    nx, ny = x + dx, y + dy
                    if 0 <= nx < self.nrow and 0 <= ny < self.ncol and self.present_state[nx, ny]!=-2:
                        pred+=1
                if pred > 0 :
                    reward = self.reward_dict['empty'] * 3
                else:
                    reward = self.reward_dict['empty']
                done = False
       ```
        
ver1 ë³´ë‹¤ ver2 ì˜ ë³´ìƒ ì²´ê³„ê°€ ê¸°ê³„ì˜ ì§€ì  ì¶”ë¡ ì— ë„ì›€ì´ ë  ê²ƒì´ë¼ íŒë‹¨í•˜ê³  ver1 ìœ¼ë¡œ 400000 episode í•™ìŠµì‹œí‚¨ ê²ƒì„ ver2 ë¡œ ë°”ê¾¸ì–´ 250000 ì •ë„ ì¶”ê°€í•™ìŠµ ì‹œì¼°ë‹¤.
   
<br>

#### ìŒìˆ˜ ë³´ìƒ ì²´ê³„

|  | mine | 
| --- | --- | 
| reward | -8 |
| done | True |

- ìŒì˜ ë³´ìƒì„ ì£¼ëŠ” ê²½ìš°ëŠ” ì§€ë¢°ë¥¼ ë°ŸëŠ” ê²½ìš°ë¡œ ì„¤ì •ì„ í•˜ì˜€ë‹¤.
- `mine` ì˜ ê²½ìš°ì— ìŒì˜ ë³´ìƒì„ ë„ˆë¬´ í¬ê²Œ ì£¼ë©´, ì „ì²´ì˜ ë³´ìƒì´ ìŒìˆ˜ê°€ ë˜ëŠ”ë° ì´ ë•Œ í•™ìŠµ ì†ë„ê°€ ì €í•˜ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì—¬, ì´ reward ì˜ í•©ì´ ì–‘ìˆ˜ê°€ ë‚˜ì˜¤ë„ë¡ ì ë‹¹í•œ ìŒì˜ ë³´ìƒê°’ì„ ì„¤ì •í•˜ì˜€ë‹¤.

## 2. DQN Net

- DQN Net ì€ 4ê°œì˜ CNN ì¸µ, 1ê°œì˜ fc ì¸µìœ¼ë¡œ ì„¤ê³„í•˜ì˜€ë‹¤. batch normalization ì˜ ê²½ìš°, 4ê°œì˜ CNN ì¸µì— ëª¨ë‘ ì ìš©í•˜ëŠ” ê²ƒë³´ë‹¤, ì¼ë¶€ì—ë§Œ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì´ ì´ˆê¸°ì— ì„±ëŠ¥ì´ ìˆ˜ë ´í•˜ëŠ”ë° ë” ì¢‹ë‹¤ëŠ” ê²ƒì„ ê²½í—˜ì ìœ¼ë¡œ ë°œê²¬í•˜ì˜€ë‹¤. ë˜í•œ, a* ì•Œê³ ë¦¬ì¦˜ì— ê¸°ë°˜í•œ classification task ì— ì“°ì´ëŠ” fc ì¸µ ì§€ìˆ˜ìŠ¹ ê°ì†Œê°€ ì˜¤íˆë ¤ ì´ task ì—ì„œ ì„±ëŠ¥ì´ ì•ˆ ì¢‹ë‹¤ëŠ” ê²ƒ ë˜í•œ ë°œê²¬í•˜ì—¬ fc layer ëŠ” í•˜ë‚˜ì˜ ì¸µìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.
  
```python
class DQN_Net(nn.Module):
    def __init__(self, state, action_size, conv_units=64):
        super().__init__()
        self.state_size = len(state) * len(state)
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
        self.bn1 = nn.BatchNorm2d(conv_units)
        self.conv2 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
        self.conv3 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
        self.conv4 = nn.Conv2d(in_channels=conv_units, out_channels=conv_units, kernel_size=(3,3), bias=False, padding=1)
        self.fc1 = nn.Linear(5184, action_size)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn1(self.conv2(x)))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))

        # flatten
        x = x.view(-1, 5184)

        x = self.fc1(x)
        return x
```

## 3. CONFIG
DQN Agent ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ CONFIG ì„¤ì • ê°’ì´ë‹¤. ì•½ 4300 EPISODE ì •ë„ íƒìƒ‰ì„ í•  ìˆ˜ ìˆë„ë¡ `EPSILON`, `EPSILON_DECAY`, `EPSILON_MIN` ê°’ì„ ì„¤ì •í•˜ì˜€ë‹¤. ë˜í•œ, LEARN_MAX ë¥¼ 0.005ë¡œ í¬ê²Œ ì„¤ì •í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë°, ì´ˆê¸°ì— í° Learning rate ê°€ ìˆ˜ë ´ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ê¸° ë•Œë¬¸ì´ë‹¤.

```python
MEM_SIZE = 50000
MEM_SIZE_MIN = 1000

BATCH_SIZE = 64
GAMMA = 0.1 #gamma


LEARN_MAX = 0.005
LEARN_MIN = 0.00005
LEARN_EPOCH = 2500
LEARN_DECAY = 0.99


EPSILON = 0.9997
EPSILON_DECAY = 0.9997
EPSILON_MIN = 0.001

CONV_UNITS = 64
DENSE_UNITS = 512
UPDATE_TARGET_EVERY = 5

EPISODES = 60000
PRINT_EVERY=100
SAVE_EVERY=1000
```

## 4. DQN Agent

DQN ì—ì„œ í–‰ë™ì„ ì„ íƒí•  ë•Œ, 2ê°€ì§€ì˜ ê²½ìš°ê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” íƒìƒ‰ìœ¼ë¡œ ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ë¥¼ ë½‘ëŠ” ê²ƒì´ë‹¤. í˜„ ëª¨ë¸ì„ Rule based ì´ê¸° ë•Œë¬¸ì— ë°Ÿì§€ ì•Šì€ ì¢Œí‘œ ì¤‘ ì•„ë¬´ê±°ë‚˜ í•˜ë‚˜ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ê³„í•˜ì˜€ë‹¤.
ë‚˜ë¨¸ì§€ í•˜ë‚˜ëŠ” íƒìš• ì •ì±…ì„ ë”°ë¥´ëŠ” ê²ƒì¸ë°, state ë¥¼ ì…ë ¥ìœ¼ë¡œ ì£¼ì—ˆì„ ë•Œ ê°€ì¥ í° q ê°’ì„ ê°€ì§„ action ì„ ë°˜í™˜í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ë°Ÿì§€ ì•Šì€ ì¢Œí‘œ ì¤‘ ê°€ì¥ í° q ê°’ì„ ê°€ì§€ëŠ” ê²ƒì„ ë°˜í™˜í•˜ë„ë¡ í•˜ì˜€ë‹¤. ì´ ë•Œ, ê°€ì¥ í° ê°’ì„ êµ¬í•˜ê¸° ìœ„í•˜ì—¬ ë°Ÿì€ ê³³ì€ -inf ë§ˆìŠ¤í‚¹ì„ í•´ì£¼ì–´ ì–‘ìˆ˜, ìŒìˆ˜ êµ¬ë¶„ì„ ë”°ë¡œ í•˜ì§€ ì•Šê³  í•œë²ˆì— ìµœëŒ“ê°’ì„ ë°˜í™˜í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ì‹œê°„ë³µì¡ë„ë¥¼ ë‚®ì¶”ì—ˆë‹¤.

```python
def get_action(self, state):
        state = torch.tensor(state)
        if np.random.rand() <= self.epsilon:  # ë¬´ì‘ìœ„ íƒìƒ‰
            valid_indices = (state == -2).nonzero(as_tuple=True)

            rand_idx = np.random.randint(len(valid_indices[0]))
            x, y = valid_indices[0][rand_idx].item(), valid_indices[1][rand_idx].item()

            return x, y

        else: # ê°€ì¥ í° q ê°’ì˜ action ì¶”ì¶œ
            state = state.unsqueeze(0).unsqueeze(0).to(dtype=torch.float).to(device) 
            with torch.no_grad():
                q_values = self.model(state).flatten()

                flat = state.squeeze(0).flatten()
                mask = (flat == -2).float()

                masked_q_values = q_values * mask
                masked_q_values[mask == 0] = float('-inf')

                max_q_value, action_idx = torch.max(masked_q_values, dim=-1)

                x = action_idx // self.n
                y = action_idx % self.n

                self.action = [item for item in self.action if item != (x, y)]
                return x, y
```

train_model í•¨ìˆ˜

Rule based ëŠ” No Rule based ì™€ train_model ì„ ë‹¤ë¥´ê²Œ ì„¤ê³„í•˜ì˜€ë‹¤. <br>
Rule based ì—ì„  next_state ë¥¼ input ìœ¼ë¡œ ë°›ê³  ìµœëŒ€ q ê°’ì„ ë°˜í™˜í•˜ëŠ” target model ì—ë„ ë°Ÿì€ ê³³ì€ ë˜ ë°Ÿì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ë§ˆìŠ¤í‚¹ ì‘ì—…ì„ í•˜ì˜€ë‹¤. 

- ì˜ˆì¸¡ ê°’ìœ¼ë¡œ Q(s, a)ê°’ ì‚¬ìš©

```python
with torch.no_grad():
            next_q_values = self.target_model(next_states).flatten(1)
            flat_next_states = next_states.squeeze(1).flatten(1)
            valid_mask = (flat_next_states == -2).float()
            valid_next_q_values = next_q_values * valid_mask
            valid_next_q_values[valid_mask == 0] = float('-inf')
            max_valid_q_values, _ = torch.max(valid_next_q_values, dim=-1)
            target_q_values = rewards + (torch.ones_like(dones) - dones) * GAMMA * max_valid_q_values

```

- íƒ€ê²Ÿ ê°’ ê³„ì‚°: $reward + (1-done)\times gamma\times Q(s', a')$

```python
loss = F.mse_loss (predicts, target_q_values)
```

- ì˜¤ë¥˜ í•¨ìˆ˜ (mse)ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸

```python
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
```

- CONFIG ì—ì„œ ì„¤ì •í•œ LEARN_EPOCH ë§ˆë‹¤ LEARN_DECAY ë§Œí¼ lr ê°ì‡ 

```python
if (episode+1) % LEARN_EPOCH == 0:
    lr = agent.optimizer.param_groups[0]['lr'] * LEARN_DECAY
    agent.optimizer.param_groups[0]['lr'] = max(lr, LEARN_MIN)
```
## 5. ê²°ê³¼

### train
- 65ë§Œ episode ì¤‘ í›„ê¸° 15ë§Œ episode ì˜ ì„±ëŠ¥ ì§€í‘œì´ë‹¤.
- ì•½ 5ë§Œ episode ë§ˆë‹¤ median cnt ê°€ í•˜ë‚˜ì”© ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìœ¼ë©°, avg clear ë¡œ ë³´ì•„ train ì‹œ ì„±ëŠ¥ì´ ì•½ 30% ëŒ€ì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
![image](https://github.com/user-attachments/assets/353447a6-7797-42af-b903-4f909b0a6377)
![image](https://github.com/user-attachments/assets/ddaccbc4-6390-4ed7-8f03-acbbde1d1d81)
![image](https://github.com/user-attachments/assets/8983366f-161e-45d5-a8ce-e51f080483ea)

### test
- test ëŠ” model ì„ evaluation ëª¨ë“œë¡œ ë°”ê¾¸ê³ , íƒìƒ‰í•˜ëŠ” ê³¼ì •ì„ ì—†ì•´ë‹¤.
- 1ë§Œ EPISODE ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê³ , ì„±ëŠ¥ì„ 100 EPISODE ë§ˆë‹¤ ê¸°ë¡í•˜ì˜€ë‹¤. Rule based ì‹œ, ìµœëŒ€ 100 ë²ˆ í…ŒìŠ¤íŠ¸ ë‹¹ 47 ë²ˆ `CLEAR` í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆê³  í‰ê·  ì•½ 28% ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.
  ![image](https://github.com/user-attachments/assets/b5ab6e3b-abea-4d6b-ba26-d1cc6937e52e)

